# UNSW-NB15 Data Preprocessing
# Concise implementation for Google Colab

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

class UNSWNB15Preprocessor:
    def __init__(self):
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.imputer = SimpleImputer(strategy='median')
        self.pca = PCA(n_components=0.95)  # Keep 95% variance
        
    def load_csv(self, file_path):
        """Load UNSW-NB15 dataset from CSV file"""
        try:
            # Load CSV and automatically detect columns
            df = pd.read_csv(file_path)
            
            # Auto-detect target column (usually 'label' or 'attack_cat' for UNSW-NB15)
            target_candidates = ['label', 'attack_cat', 'Label', 'Attack_cat']
            target_col = None
            
            for col in target_candidates:
                if col in df.columns:
                    target_col = col
                    break
            
            # If no standard target found, assume last column
            if target_col is None:
                target_col = df.columns[-1]
            
            # Rename target column for consistency
            if target_col != 'attack_type':
                df = df.rename(columns={target_col: 'attack_type'})
            
            print(f"Dataset loaded: {df.shape}")
            print(f"Target column: {target_col} -> attack_type")
            print(f"Features: {len(df.columns)-1}")
            
            return df
            
        except Exception as e:
            print(f"Error loading CSV: {e}")
            return None
    
    def clean_data(self, df):
        """Basic data cleaning for UNSW-NB15"""
        # Remove duplicates
        df = df.drop_duplicates()
        
        # Handle infinite values
        df = df.replace([np.inf, -np.inf], np.nan)
        
        # Remove non-informative columns (if exist)
        drop_cols = ['id', 'Id', 'ID'] 
        df = df.drop(columns=[col for col in drop_cols if col in df.columns])
        
        return df
    
    def encode_categorical(self, df):
        """Encode categorical features"""
        categorical_cols = df.select_dtypes(include=['object']).columns
        categorical_cols = [col for col in categorical_cols if col != 'attack_type']
        
        for col in categorical_cols:
            df[col] = pd.Categorical(df[col]).codes
        
        return df
    
    def encode_labels(self, labels, fit=True):
        """Encode attack labels for UNSW-NB15"""
        if fit:
            return self.label_encoder.fit_transform(labels)
        else:
            return self.label_encoder.transform(labels)
    
    def normalize_features(self, df, fit=True):
        """Normalize numerical features"""
        numerical_cols = df.select_dtypes(include=[np.number]).columns
        
        if fit:
            df[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])
        else:
            df[numerical_cols] = self.scaler.transform(df[numerical_cols])
        
        return df
    
    def handle_missing_values(self, df, fit=True):
        """Handle missing values"""
        if fit:
            df_imputed = pd.DataFrame(self.imputer.fit_transform(df), 
                                    columns=df.columns, index=df.index)
        else:
            df_imputed = pd.DataFrame(self.imputer.transform(df), 
                                    columns=df.columns, index=df.index)
        return df_imputed
    
    def reduce_dimensionality(self, X, fit=True):
        """Apply PCA for dimensionality reduction"""
        if fit:
            X_reduced = self.pca.fit_transform(X)
        else:
            X_reduced = self.pca.transform(X)
        
        print(f"PCA: {X.shape[1]} -> {X_reduced.shape[1]} features")
        return X_reduced
    
    def save_processed_data(self, X, y, prefix="processed"):
        """Save preprocessed data to CSV files"""
        try:
            # Convert to DataFrames
            if hasattr(X, 'shape'):
                X_df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])
            else:
                X_df = pd.DataFrame(X)
            
            y_df = pd.DataFrame(y, columns=['attack_type'])
            combined_df = pd.concat([X_df, y_df], axis=1)
            
            # Save to CSV
            output_path = f"/content/{prefix}_unsw_data.csv"
            combined_df.to_csv(output_path, index=False)
            
            print(f"Saved: {output_path}")
            return output_path
            
        except Exception as e:
            print(f"Error saving: {e}")
            return None
    
    def preprocess(self, df, target_col='attack_type', fit=True, apply_pca=True):
        """Complete preprocessing pipeline"""
        print(f"Original shape: {df.shape}")
        
        # Separate features and target
        if target_col in df.columns:
            X = df.drop(target_col, axis=1)
            y = df[target_col]
        else:
            X = df.iloc[:, :-1]
            y = df.iloc[:, -1]
        
        # Preprocessing steps
        X = self.clean_data(X)
        X = self.handle_missing_values(X, fit=fit)
        X = self.encode_categorical(X)
        X = self.normalize_features(X, fit=fit)
        
        if apply_pca:
            X = self.reduce_dimensionality(X, fit=fit)
        
        # Encode labels
        y_encoded = self.encode_labels(y, fit=fit)
        
        print(f"Processed shape: {X.shape}")
        print(f"Classes: {len(np.unique(y_encoded))}")
        
        return X, y_encoded

def preprocess_unsw_data(csv_file_path, save_output=True):
    """Main function to preprocess UNSW-NB15 data"""
    # Initialize preprocessor
    preprocessor = UNSWNB15Preprocessor()
    
    # Load CSV
    df = preprocessor.load_csv(csv_file_path)
    
    if df is not None:
        # Split train/test (80/20)
        train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, 
                                           stratify=df['attack_type'])
        
        # Preprocess
        X_train, y_train = preprocessor.preprocess(train_df, fit=True, apply_pca=True)
        X_test, y_test = preprocessor.preprocess(test_df, fit=False, apply_pca=True)
        
        print("\n=== UNSW-NB15 Preprocessing Complete ===")
        print(f"Training: {X_train.shape}")
        print(f"Testing: {X_test.shape}")
        print(f"Classes: {preprocessor.label_encoder.classes_}")
        
        # Save processed data
        if save_output:
            train_path = preprocessor.save_processed_data(X_train, y_train, "train")
            test_path = preprocessor.save_processed_data(X_test, y_test, "test")
            print(f"\nFiles saved:\n- {train_path}\n- {test_path}")
        
        return X_train, y_train, X_test, y_test, preprocessor
    
    return None

# Usage
if __name__ == "__main__":
    # Replace with your CSV file path
    csv_path = "/content/your_unsw_file.csv"
    data = preprocess_unsw_data(csv_path)
